  <script>self.__next_f.push([
    1,
    "# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [memory-bank/activeContext.md](memory-bank/activeContext.md)\n\n\u003c/details\u003e\n\n\n\nThe LLM Literary Analysis project is a system designed to evaluate the literary understanding capabilities of various Large Language Models (LLMs). It provides a structured framework for analyzing how well different AI models interpret and understand literary texts, with particular focus on sentiment analysis, including emotions like interest, surprise, sadness, and anger.\n\nThis page introduces the high-level architecture, key components, and core functionality of the system. For detailed information about specific models, see [Supported Models](#1.1), and for information about specific features, see [Key Features](#1.2).\n\n## System Purpose and Scope\n\nThe LLM Literary Analysis system enables researchers to conduct standardized experiments across multiple LLM providers to assess their literary comprehension capabilities. The system:\n\n- Evaluates multiple LLMs from various providers (OpenAI, Google, Anthropic, X.AI, DeepSeek, Meta, Alibaba)\n- Performs sentiment analysis on literary texts\n- Provides batch processing capabilities for efficient experimentation\n- Implements intelligent temperature control based on persona characteristics\n- Ensures consistent formatting and validation of results\n- Offers aggregation and analysis tools for experimental data\n\nSources: [README.md:3-5](), [README.md:8-16]()\n\n## System Architecture\n\n```mermaid\ngraph TD\n    subgraph \"User Interface\"\n        CLI[\"Command Line Interface\"]\n    end\n\n    subgraph \"Experiment Control\"\n        ModelSelection[\"Model Selection Module\"]\n        BatchProcessing[\"Batch Processing System\"]\n    end\n\n    subgraph \"Experiment Execution\"\n        ExperimentRunners[\"Experiment Runners\"]\n        PromptManager[\"Prompt Manager\"]\n        ParametersConfig[\"Parameters Configuration\"]\n    end\n\n    subgraph \"API Integration\"\n        ApiClients[\"LLM Provider APIs\"]\n    end\n\n    subgraph \"Result Management\"\n        ResultProcessing[\"Result Processing\"]\n        ResultStorage[\"Results Storage\"]\n        ResultAggregation[\"Result Aggregation\"]\n    end\n\n    CLI --\u003e ModelSelection\n    CLI --\u003e BatchProcessing\n    ModelSelection --\u003e ExperimentRunners\n    BatchProcessing --\u003e ExperimentRunners\n    ParametersConfig --\u003e ExperimentRunners\n    ParametersConfig --\u003e PromptManager\n    PromptManager --\u003e ExperimentRunners\n    ExperimentRunners --\u003e ApiClients\n    ApiClients --\u003e ResultProcessing\n    ResultProcessing --\u003e ResultStorage\n    ResultStorage --\u003e ResultAggregation\n```\n\nThe system follows a modular architecture where users interact through a command-line interface, selecting models or initiating batch processing. Experiment runners execute tests using configuration parameters and prompts managed by the Prompt Manager. Results are processed, stored, and can be aggregated for analysis.\n\nSources: [README.md:75-112]()\n\n## Core Components\n\n### Experiment Runners\n\nThe system includes specialized experiment runners for each LLM provider:\n\n| Provider | Experiment Runner | Supported Models |\n|----------|-------------------|------------------|\n| Google | GeminiExperimentRunner | Gemini 2.5/2.0 series |\n| OpenAI | OpenAIExperimentRunner | GPT-4.1/GPT-4 series |\n| Anthropic | ClaudeExperimentRunner | Claude 3.7/3.5/3.0 series |\n| X.AI | GrokExperimentRunner | Grok 2.0 series |\n| DeepSeek | DeepSeekExperimentRunner | R1/V3 series |\n| Meta | LlamaExperimentRunner | 4.0/3.3 series |\n| Alibaba | QwenExperimentRunner | Qwen models |\n\nEach runner implements provider-specific logic while inheriting common functionality from a base class.\n\nSources: [README.md:17-42]()\n\n### Batch Processing Systems\n\n```mermaid\ngraph TD\n    subgraph \"Batch Processing System\"\n        BatchControl[\"Batch Control Module\"]\n        \n        subgraph \"Provider-Specific Implementations\"\n            OpenAIBatch[\"OpenAI Batch Runner\"]\n            ClaudeBatch[\"Claude Batch Runner\"]\n            KlusterBatch[\"kluster.ai Batch Runner\"]\n        end\n        \n        subgraph \"Optimization Features\"\n            CostOpt[\"Cost Optimization\"]\n            ErrorHandling[\"Error Handling\"]\n            Validation[\"Result Validation\"]\n        end\n    end\n    \n    BatchControl --\u003e OpenAIBatch \u0026 ClaudeBatch \u0026 KlusterBatch\n    OpenAIBatch \u0026 ClaudeBatch \u0026 KlusterBatch --\u003e CostOpt\n    OpenAIBatch \u0026 ClaudeBatch \u0026 KlusterBatch --\u003e ErrorHandling\n    ErrorHandling --\u003e Validation\n    \n    OpenAIBatch --\u003e OpenAIF[\"JSONL Format\"]\n    ClaudeBatch --\u003e ClaudeF[\"Message Batches API\"]\n    KlusterBatch --\u003e KlusterF[\"OpenAI-compatible API\"]\n```\n\nThe system implements specialized batch processing for different LLM providers, optimizing for cost efficiency and handling provider-specific APIs and formats.\n\nSources: [README.md:76-89](), [memory-bank/activeContext.md:23-38]()\n\n### Prompt Management\n\nThe Prompt Manager handles the creation and adaptation of prompts for different models, incorporating:\n- Base prompts for literary analysis\n- System prompts that define personas\n- Temperature adjustment based on persona characteristics and text types\n- Model-specific prompt formatting\n\nSources: [README.md:124-129](), [memory-bank/activeContext.md:24-27]()\n\n### Configuration System\n\nThe Parameters module provides centralized configuration including:\n- Base and system prompts\n- Model configurations\n- Persona definitions\n- Literary texts for analysis\n- Temperature settings\n\nSources: [README.md:124-129]()\n\n### Result Management\n\n```mermaid\nflowchart LR\n    subgraph \"Result Processing Flow\"\n        Response[\"LLM Response\"]\n        Validation[\"Response Validation\"]\n        Extraction[\"Value \u0026 Reason Extraction\"]\n        Storage[\"Result Storage\"]\n        Aggregation[\"Result Aggregation\"]\n        Analysis[\"Analysis \u0026 Visualization\"]\n    end\n    \n    Response --\u003e Validation\n    Validation --\u003e Extraction\n    Extraction --\u003e Storage\n    Storage --\u003e Aggregation\n    Aggregation --\u003e Analysis\n    \n    subgraph \"Storage Structure\"\n        ResultsDir[\"results/\"]\n        ModelDirs[\"Model-specific directories\"]\n        ResultFiles[\"Standardized result files\"]\n    end\n    \n    Storage --\u003e ResultsDir\n    ResultsDir --\u003e ModelDirs\n    ModelDirs --\u003e ResultFiles\n```\n\nThe system processes LLM responses through validation, extraction of values and reasons, standardized storage, and aggregation for analysis.\n\nSources: [README.md:130-137](), [memory-bank/activeContext.md:70-87]()\n\n## Experiment Execution Flow\n\n```mermaid\nflowchart TD\n    Start[\"Start Experiment\"] --\u003e SelectMode{\"Single or Batch?\"}\n    \n    SelectMode --\u003e|\"Single\"| ModelSelect[\"Select Model(s)\"]\n    SelectMode --\u003e|\"Batch\"| BatchSetup[\"Setup Batch Process\"]\n    \n    ModelSelect --\u003e InitRunner[\"Initialize Experiment Runner\"]\n    BatchSetup --\u003e InitBatch[\"Initialize Batch Runner\"]\n    \n    InitRunner \u0026 InitBatch --\u003e LoadConfig[\"Load Configuration Parameters\"]\n    \n    LoadConfig --\u003e IterateCombinations[\"Iterate through combinations of\\nmodels, personas, and texts\"]\n    \n    subgraph \"For each combination\"\n        IterateCombinations --\u003e GeneratePrompt[\"Generate Prompt\"]\n        GeneratePrompt --\u003e CalculateTemp[\"Calculate Temperature\"]\n        CalculateTemp --\u003e ExecuteAPI[\"Execute API Call\"]\n        ExecuteAPI --\u003e ProcessResponse[\"Process Response\"]\n        ProcessResponse --\u003e SaveResult[\"Save Result\"]\n    end\n    \n    SaveResult --\u003e MoreCombinations{\"More combinations?\"}\n    MoreCombinations --\u003e|\"Yes\"| IterateCombinations\n    MoreCombinations --\u003e|\"No\"| End[\"End Experiment\"]\n```\n\nThe experiment execution follows a structured flow, from selecting the mode (single or batch) to iterating through combinations of models, personas, and texts, generating prompts, calculating appropriate temperatures, executing API calls, and saving results.\n\nSources: [README.md:74-112]()\n\n## Component Relationships\n\n```mermaid\nclassDiagram\n    class \"BaseExperimentRunner\" {\n        +run_experiment()\n        +extract_value()\n        +extract_reason()\n        +save_result()\n    }\n    \n    class \"Parameters\" {\n        +BASE_PROMPT\n        +SYSTEM_PROMPTS\n        +MODEL_CONFIGS\n        +PERSONAS\n        +TEXTS\n    }\n    \n    class \"PromptManager\" {\n        +get_prompt()\n        -_calculate_temperature()\n        -_get_base_prompt()\n        -_get_system_prompt()\n        -_adapt_for_model()\n    }\n    \n    class \"ResultProcessing\" {\n        +process_response()\n        +validate_response()\n        +extract_data()\n        +save_to_file()\n    }\n    \n    BaseExperimentRunner \u003c|-- \"GeminiExperimentRunner\"\n    BaseExperimentRunner \u003c|-- \"ClaudeExperimentRunner\" \n    BaseExperimentRunner \u003c|-- \"OpenAIExperimentRunner\"\n    BaseExperimentRunner \u003c|-- \"GrokExperimentRunner\"\n    BaseExperimentRunner \u003c|-- \"DeepSeekExperimentRunner\"\n    BaseExperimentRunner \u003c|-- \"LlamaExperimentRunner\"\n    BaseExperimentRunner \u003c|-- \"QwenExperimentRunner\"\n    \n    BaseExperimentRunner --\u003e Parameters\n    BaseExperimentRunner --\u003e PromptManager\n    BaseExperimentRunner --\u003e ResultProcessing\n    PromptManager --\u003e Parameters\n```\n\nThe class diagram shows the inheritance hierarchy of experiment runners and their relationships with the Parameters, PromptManager, and ResultProcessing components.\n\nSources: [README.md:17-42]()\n\n## Installation and Requirements\n\nThe system requires:\n- Python 3.12\n- Provider-specific libraries:\n  - google-generativeai\u003e=0.3.0 (for Gemini models)\n  - anthropic\u003e=0.43.0 (for Claude models)\n  - openai\u003e=1.0.0 (for Grok and OpenAI models)\n  - python-dotenv\u003e=1.0.0 (for environment variables)\n\nInstallation follows standard Python package procedures:\n1. Clone the repository\n2. Install dependencies via pip\n3. Configure API keys in environment variables\n\nSources: [README.md:45-70]()\n\n## Command Line Usage\n\nUsers interact with the system through command-line interfaces that provide options for:\n\n- Model selection: Choose specific models for experimentation\n  ```\n  python openai_example.py --model gpt-4.1 gpt-4.1-mini\n  ```\n\n- Batch processing: Run experiments in batch mode for efficiency\n  ```\n  python openai_example.py --batch\n  ```\n\n- Combined usage: Select specific models for batch processing\n  ```\n  python openai_example.py --batch --model gpt-4.1 gpt-4.1-mini\n  ```\n\nSources: [README.md:74-112]()\n\n## Summary\n\nThe LLM Literary Analysis system provides a comprehensive framework for evaluating the literary understanding capabilities of various LLMs. Through a modular architecture, it supports multiple model providers, efficient batch processing, intelligent prompt management, and structured result analysis.\n\nFor more detailed information about specific aspects of the system, refer to the following pages:\n- [Supported Models](#1.1) - Details on all supported LLM models\n- [Key Features](#1.2) - Information about sentiment analysis, batch processing, and temperature control\n- [System Architecture](#2) - In-depth explanation of system components and their interactions\n- [Experiment Execution](#3) - Detailed workflows for running experiments\n- [Result Management](#4) - How experiment results are stored, processed, and analyzed\n\nSources: [README.md:1-16]()"
  ])</script>